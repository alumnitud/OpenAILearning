- LLM: like an advanced form of autocomplete. Built on deep learning architectures like transformers (made training process faster using parallel processing).
- 1st layer of LLM: Word embeddings (vector representations of words). Read about Mikolov's paper- Word2Vec model.
- BERT model (Bidirectional Encoder Representations from Transformers): During pretraining, the model is given sentences with some words which are masked. It looks in both directions (left and right) for context 
and tries to fill up the masked words. This makes it better at capturing the context. 
- GPT: autoregressive model (predicts future values-tokens- from past values). These models can generalize pretty well due to the large param size, so no need of finetuning for a specific scenario.
This token-by-token calculation results in coherent text output, but promblem: if 1 token has an error, then that error keeps continuing.
- Usually finetuning was used to tailor models to specific tasks, but with large models like GPT --> Prompt Engineering.

3 Training Approaches:
1) Causal Language Modelling (CLM): autoregressive method- look at past values to predict next token. Useful for text generation + summarization
2) Masked Language Modelling (MLM): some random tokens are masked and model predicts these words based on left and right context. Useful for text classification, sentiment analysis, NEM. Not suitable for text 
generation as in that case it should ignore right context.
3) Sequence-to-Sequence (Seq2Seq): encoder-decoder architecture- Useful for clear input/output transformation tasks, e.g. translation.

Transformers:
- Paper: Attention is all you need.
- Capture long range dependencies, uses parallelization.
- "self-attention mechanism": ?
